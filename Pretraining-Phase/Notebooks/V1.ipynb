{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87f8c23",
      "metadata": {
        "id": "a87f8c23"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17de235d",
      "metadata": {
        "id": "17de235d"
      },
      "outputs": [],
      "source": [
        " ### Sample to just understand the multihead\n",
        "\n",
        "context_length = 5 ## just a sentence length\n",
        "batch_size = 1\n",
        "n_embd = 12\n",
        "n_head = 3\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3038181f",
      "metadata": {
        "id": "3038181f"
      },
      "source": [
        "### Two Approaches to Multi-Head Attention\n",
        "\n",
        "#### 1. Single Linear Layer (Industry Standard Approach)\n",
        "- In this method, **one Linear layer** generates **Q, K, and V together**.\n",
        "- The layer projects the input from `d_model ‚Üí 3 √ó d_model` in a single forward pass.\n",
        "- After that, the output is **reshaped and split into multiple heads**.\n",
        "- This approach is **fast and efficient**, which is why it is used in almost all production-grade Transformer models (e.g., GPT, BERT, etc.).\n",
        "\n",
        "#### 2. Separate Per-Head Linear Layers (Parallel Head Construction)\n",
        "- In this method, **each attention head has its own independent Linear layers** for Q, K, and V.\n",
        "- Heads operate **independently**, and their outputs are **combined at the end**.\n",
        "- This approach is **conceptually simpler**, but **slower** because:\n",
        "  - More linear layers\n",
        "  - More parameter copies\n",
        "  - Less efficient GPU batching\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why Approach 1 is Preferred\n",
        "\n",
        "1. **Efficiency / Speed**\n",
        "   - One linear layer for QKV means **fewer matrix multiplications** than separate layers.\n",
        "   - Computation is done in a **single large matrix operation**, reducing overhead and improving GPU parallelism.\n",
        "\n",
        "2. **Industry Usage**\n",
        "   - Most production models (e.g., GPT, BERT) use this method because it is **fast and memory-efficient**.\n",
        "   - The concatenated heads‚Äô weights are implemented with a **single linear layer** for simplicity and scalability.\n",
        "\n",
        "3. **Memory Efficiency**\n",
        "   - Fewer linear layers ‚Üí **fewer parameters** ‚Üí **less memory usage**, which is critical for large-scale models.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùó Caveats / When Separate Heads Might Make Sense\n",
        "\n",
        "- Separate per-head layers offer **more flexibility**: each head can learn **very different projections**.\n",
        "- For **small models or teaching purposes**, the performance difference is **minimal**.\n",
        "- **Specialized architectures** or **memory-constrained inference** may use variants like **multi-query attention**.\n",
        "- The single-linear approach **assumes all heads have equal dimension**; if you need **different head sizes**, separate layers are required.\n",
        "\n",
        "---\n",
        "\n",
        "### üîé Summary / Conclusion\n",
        "\n",
        "- ‚úÖ **Single Linear for QKV** is widely used in industry: **faster**, **memory-efficient**, and **scalable**.\n",
        "- ‚ö†Ô∏è **Separate per-head layers** are **slower** and mostly used for **teaching** or **research experiments**.\n",
        "- While **Approach 1 dominates in practice**, **Approach 2** can be useful for **custom or experimental models**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd856761",
      "metadata": {
        "id": "bd856761"
      },
      "source": [
        "## First approach to build the Multi Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02758669",
      "metadata": {
        "id": "02758669"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, n_head: int, d_model: int, context_length: int, dropout: float = 0.0, log_shape: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.log_shape = log_shape\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Each attention head must get the same number of dimensions.\n",
        "        # Example: d_model = 12 and n_head = 3 ‚Üí d_head = 4\n",
        "        assert d_model % n_head == 0, \"Embedding size must be divisible by number of heads\"\n",
        "        self.d_head = d_model // n_head\n",
        "\n",
        "        # This linear layer produces Q, K, and V together.\n",
        "        # Input:  (B, T, d_model)\n",
        "        # Output: (B, T, 3*d_model)\n",
        "        # Because we need Q, K, V ‚Äî each of size d_model.\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "\n",
        "        # After attention finishes, all heads are merged back\n",
        "        # and passed through this output projection.\n",
        "        self.projection = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Causal mask:\n",
        "        # Upper triangular matrix with True above diagonal.\n",
        "        # This prevents a token from looking at future tokens.\n",
        "        self_mask = torch.triu(\n",
        "            torch.ones(context_length, context_length, dtype=torch.bool),\n",
        "            diagonal=1\n",
        "        )\n",
        "        # register_buffer ensures this mask moves with the model (GPU/CPU)\n",
        "        self.register_buffer(\"causal_mask\", self_mask)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        '''\n",
        "        Here we have 3 self-attention heads.\n",
        "        Each head needs its own Q, K, V vectors, and all of them must have the same size.\n",
        "\n",
        "        1. We pass 1 sentence ‚Üí it has 5 tokens ‚Üí each token has a 12-dimensional embedding.\n",
        "        So the input shape is: (1, 5, 12)\n",
        "\n",
        "        2. The qkv() layer is a Linear layer that creates all three matrices:\n",
        "            Q, K, and V\n",
        "        It does this by projecting the input from:\n",
        "            d_model ‚Üí 3 * d_model\n",
        "        So output shape becomes: (1, 5, 36)\n",
        "\n",
        "        3. Now the 36 channels can be evenly divided into 3 heads.\n",
        "        Because:\n",
        "            d_model = 12\n",
        "            n_heads = 3\n",
        "            head_dim = 12 / 3 = 4\n",
        "\n",
        "        4. Therefore, each head will receive:\n",
        "            Q of size 4,\n",
        "            K of size 4,\n",
        "            V of size 4\n",
        "        and this is repeated for all 3 heads.\n",
        "\n",
        "        In short:\n",
        "        - Input:       (1, 5, 12)\n",
        "        - After qkv(): (1, 5, 36)\n",
        "        - Split into heads: 3 heads √ó 4 dims each for Q, K, V\n",
        "        '''\n",
        "\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.view(B, T, 3, self.n_head, self.d_head)\n",
        "\n",
        "        if self.log_shape:\n",
        "            print(f\"Shape of QKV: {qkv.shape}\")\n",
        "\n",
        "        # Step 3: Split into Q, K, V\n",
        "        # Each has shape (B, T, n_head, d_head)\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "\n",
        "        '''\n",
        "    bcz  at present -> (B, T, n_head, d_head). and the multihead attention wants the (B, n_head, T, d_head)\n",
        "\n",
        "    q = (B, heads, T, d_head)\n",
        "    k = (B, heads, T, d_head)\n",
        "    v = (B, heads, T, d_head)\n",
        "\n",
        "    Each attention head now has access to all T tokens, each represented as a d_head-dim vector.\n",
        "\n",
        "    '''\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        if self.log_shape:\n",
        "            print(\"q:\", q.shape, \"k:\", k.shape, \"v:\", v.shape)\n",
        "\n",
        "        # scale factor = 1 / sqrt(d_head)\n",
        "        scale = 1.0 / math.sqrt(self.d_head)\n",
        "\n",
        "\n",
        "        '''\n",
        "        k.transpose(-2, -1):  (B, heads, d_head, T)\n",
        "\n",
        "        (B,heads,T,d_head)\n",
        "            @\n",
        "        (B,heads,d_head,T)\n",
        "        --------------------------------\n",
        "        ‚Üí (B, heads, T, T)\n",
        "\n",
        "        attention score matrix of T*T :->  each head computes a 5*5 matrix\n",
        "            scores[i,j] = how much token i attends to token j\n",
        "\n",
        "        scake = 1 / sqrt(d_head)   ->  This avoids very large dot-products which would make softmax explode\n",
        "\n",
        "        '''\n",
        "        attention_weight = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
        "\n",
        "\n",
        "        #  token positions by setting them to -inf\n",
        "        attention_weight = attention_weight.masked_fill(\n",
        "            self.causal_mask[:T, :T],\n",
        "            float('-inf')\n",
        "        )\n",
        "\n",
        "        #  Convert scores to probabilities\n",
        "        # Softmax applied on last dim so each row sums to 1\n",
        "        attention_weight_prob = F.softmax(attention_weight, dim=-1)\n",
        "        attention_weight_prob = self.dropout(attention_weight_prob)\n",
        "\n",
        "        #Multiply probabilities with V to compute context\n",
        "        # result shape = (B, n_head, T, d_head)\n",
        "        context = torch.matmul(attention_weight_prob, v)\n",
        "\n",
        "        if self.log_shape:\n",
        "            print(\"weights:\", attention_weight_prob.shape, \"context:\", context.shape)\n",
        "\n",
        "        '''\n",
        "        ctx: (B, heads, T, d_head)\n",
        "        We need to combine them into:\n",
        "        (B, T, d_model) = (1, 5, 12)\n",
        "\n",
        "\n",
        "        1. transpose back -> (B, T, heads, d_head)\n",
        "        2. reshape to (B, T, d_model)\n",
        "        '''\n",
        "        out = context.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
        "\n",
        "        # linear projection\n",
        "        out = self.projection(out)\n",
        "\n",
        "        if self.log_shape:\n",
        "            print(\"Multi-Head output:\", out.shape)\n",
        "\n",
        "        return out, attention_weight_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eb26dfd",
      "metadata": {
        "id": "4eb26dfd"
      },
      "source": [
        "## Second way to build the Multi Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02b5ca4",
      "metadata": {
        "id": "a02b5ca4"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" One head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # causal mask: lower triangular matrix\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, T, D) -> batch, token length, Dimension of one token\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        # project inputs to key, query, value\n",
        "        k = self.key(x)    # (B, T, hs)\n",
        "        q = self.query(x)  # (B, T, hs)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        attention_weight = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)   # (B, T, T)\n",
        "\n",
        "        # apply causal mask to prevent looking ahead\n",
        "        attention_weight = attention_weight.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "        # softmax to get attention probabilities\n",
        "        attention_weight_prob = F.softmax(attention_weight, dim=-1)   # (B, T, T)\n",
        "        attention_weight_prob = self.dropout(attention_weight_prob)\n",
        "\n",
        "        # weighted sum of values\n",
        "        v = self.value(x)                               # (B, T, hs)\n",
        "        out = attention_weight_prob @ v                  # (B, T, hs)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "#      nn.Sequential: layers are applied in order, automatically in forward.\n",
        "#      nn.ModuleList: just stores the layers, but you decide manually in forward how to use them (more flexible).\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    ''' Multiple heads of self-Attention in parallel '''\n",
        "\n",
        "    def __init__(self, num_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
        "        self.proj = nn.Linear(num_head*head_size , n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edbd4e69",
      "metadata": {
        "id": "edbd4e69"
      },
      "outputs": [],
      "source": [
        "vocab = 13\n",
        "d_model = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a86c9a5",
      "metadata": {
        "id": "4a86c9a5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\t‚Ä¢\tToken embeddings: Each token in the vocabulary has a unique learnable vector. Shape = (vocab_size, d_model).\n",
        "\t‚Ä¢\tExample: \"apple\" and \"orange\" get completely different learned embeddings.\n",
        "\t‚Ä¢\tPositional embeddings: Each position in the sequence (0, 1, 2, ‚Ä¶, context_length-1) has a unique vector. Shape = (context_length, d_model).\n",
        "\t‚Ä¢\tExample: position 0 always has the same embedding vector, regardless of which token is there.\n",
        "\t‚Ä¢\tIt is not per-token, its per-position.\n",
        "\n",
        " only the context length positions are trained for positional embeddings, not every token individually\n",
        "'''\n",
        "\n",
        "\n",
        "class PositionalEmbeddings(nn.Module):\n",
        "    def __init__(self, vocab: int , d_model: int, device):\n",
        "        super().__init__()\n",
        "        self.pos_embeddings = nn.Embedding(context_length,d_model, device=device)\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        pos_emb = self.pos_embeddings(x)\n",
        "        return pos_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26795a5",
      "metadata": {
        "id": "e26795a5"
      },
      "outputs": [],
      "source": [
        "pos = PositionalEmbeddings(vocab,d_model,device=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59bd0e77",
      "metadata": {
        "id": "59bd0e77"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb723303",
      "metadata": {
        "id": "fb723303",
        "outputId": "5fdf34db-1d9e-489d-9954-25aa9835a60c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: ['\\n', ' ', '(', ')', ',', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '√≠', '‡§Å', '‡§Ç', '‡§É', '‡§Ö', '‡§Ü', '‡§á', '‡§à', '‡§â', '‡§ä', '‡§è', '‡§ê', '‡§ì', '‡§î', '‡§ï', '‡§ñ', '‡§ó', '‡§ò', '‡§ô', '‡§ö', '‡§õ', '‡§ú', '‡§ù', '‡§û', '‡§ü', '‡§†', '‡§°', '‡§¢', '‡§£', '‡§§', '‡§•', '‡§¶', '‡§ß', '‡§®', '‡§™', '‡§´', '‡§¨', '‡§≠', '‡§Æ', '‡§Ø', '‡§∞', '‡§≤', '‡§µ', '‡§∂', '‡§∑', '‡§∏', '‡§π', '‡§º', '‡§Ω', '‡§æ', '‡§ø', '‡•Ä', '‡•Å', '‡•Ç', '‡•É', '‡•á', '‡•à', '‡•ã', '‡•å', '‡•ç', '‡•§', '‡••', '‡•¶', '‚Äì']\n",
            "Length of unique chars: 80\n",
            "Encoded: [49, 54, 61, 75, 45, 71]\n",
            "Decoded: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á\n"
          ]
        }
      ],
      "source": [
        "## simple tokenizer\n",
        "\n",
        "class byteTokenizer:\n",
        "    def __init__(self, file_path: str):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.text = f.read()\n",
        "\n",
        "        # Create sorted list of unique characters\n",
        "        self.chars = sorted(set(self.text))\n",
        "        print(\"Unique characters:\", self.chars)\n",
        "        print(f\"Length of unique chars: {len(self.chars)}\")\n",
        "\n",
        "        # Mapping: char -> int and int -> char\n",
        "        self.char_to_int = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.int_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self)-> int:\n",
        "        return len(self.chars)\n",
        "\n",
        "\n",
        "    def encode(self, s: str) -> list[int]:\n",
        "        \"\"\"Convert string to list of integers\"\"\"\n",
        "        return [self.char_to_int[c] for c in s]\n",
        "\n",
        "    def decode(self, l: list[int]) -> str:\n",
        "        \"\"\"Convert list of integers back to string\"\"\"\n",
        "        return ''.join([self.int_to_char[i] for i in l])\n",
        "\n",
        "\n",
        "tokenizer = byteTokenizer(\"/Users/abhaykumarsingh/Desktop/Gpt/Data/tulsidas.txt\")\n",
        "encoded = tokenizer.encode(\"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\")\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "print(\"Encoded:\", encoded)\n",
        "print(\"Decoded:\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdcb51df",
      "metadata": {},
      "source": [
        "## Summary of the Generation Pipeline of the Model\n",
        "\n",
        "\n",
        "  * **Shape:** `[vocab_size]` (e.g., 50,000 dimensions).\n",
        "  * **Values:** Unbounded real numbers (negative or positive). Higher value = higher likelihood.\n",
        "\n",
        "We don't pick the highest number immediately (Greedy Decoding) because it's repetitive. Instead, we pass these logits through a series of **filters** (Temperature, Top\\_k, Top\\_p) to reshape the probability distribution before we \"roll the dice\" (sample).\n",
        "\n",
        "\n",
        "## 1\\. Temperature ($T$): The Logit Scaler\n",
        "\n",
        "In implementation terms, Temperature is simply a **scalar division** applied to the logits *before* the Softmax function.\n",
        "\n",
        "### The Math\n",
        "\n",
        "Standard Softmax looks like this:\n",
        "$$P_i = \\frac{e^{x_i}}{\\sum e^{x_j}}$$\n",
        "\n",
        "With Temperature $T$, we modify the input $x$:\n",
        "$$P_i = \\frac{e^{x_i / T}}{\\sum e^{x_j / T}}$$\n",
        "\n",
        "### Implementation Logic\n",
        "\n",
        "1.  **Input:** A tensor of logits (e.g., `[2.0, 4.0, -1.0]`).\n",
        "2.  **Operation:** Divide the tensor by $T$.\n",
        "3.  **Output:** Pass result to Softmax.\n",
        "\n",
        "### Analysis\n",
        "\n",
        "  * **If $T < 1$ (e.g., 0.1):**\n",
        "      * You are dividing by a fraction, which acts like multiplication.\n",
        "      * The gaps between numbers expand. A distinct winner becomes a *massive* winner after the exponential function ($e^x$) is applied.\n",
        "      * **Result:** The distribution becomes \"spiky\" (low entropy). The model becomes deterministic.\n",
        "  * **If $T > 1$ (e.g., 1.5):**\n",
        "      * You are dividing by a large number. The values shrink closer to 0.\n",
        "      * $e^0 = 1$. As values get closer to 0, their exponentials become similar.\n",
        "      * **Result:** The distribution flattens (high entropy). Even \"bad\" words get a decent probability.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 2\\. Top\\_k: The Rank-Based Hard Filter\n",
        "\n",
        "Top\\_k is a **sorting and masking** operation. It ignores probability mass and looks strictly at rank.\n",
        "\n",
        "### Implementation Logic\n",
        "\n",
        "We want to keep the $k$ highest logits and zero out the rest.\n",
        "\n",
        "1.  **Sort:** Sort the logits (or probabilities) in descending order.\n",
        "2.  **Cutoff:** Identify the value at index $k$.\n",
        "3.  **Mask:** Create a boolean mask.\n",
        "      * Any value **less than** the $k$-th value is set to negative infinity ($-\\infty$).\n",
        "4.  **Renormalize:** Apply Softmax again.\n",
        "      * Because $e^{-\\infty} = 0$, those tokens now have literally 0 probability.\n",
        "      * The remaining probabilities are scaled up so they sum to 1.0 again.\n",
        "\n",
        "### trade-off with **Temperature** which controls the distribution\n",
        "\n",
        "  * **Pros:** Computationally cheap (just a sort/select). Guarantees you never sample from the \"long tail\" of garbage words.\n",
        "  * **Cons:** It is static.\n",
        "      * If the distribution is flat (many good words), $k=5$ cuts off valid options.\n",
        "      * If the distribution is peaked (only 1 good word), $k=5$ forces the model to consider 4 bad options.\n",
        "\n",
        "\n",
        "## 3\\. Top\\_p (Nucleus Sampling): The Cumulative Density Filter\n",
        "\n",
        "Top\\_p is smarter. It uses the **Cumulative Distribution Function (CDF)**. Instead of a fixed *number* of tokens, we want a fixed *mass* of probability.\n",
        "\n",
        "### Implementation Logic\n",
        "\n",
        "Let's say `top_p = 0.9`. We want the smallest set of words whose combined probability is 90%.\n",
        "\n",
        "1.  **Sort:** Sort probabilities in descending order.\n",
        "2.  **Cumsum:** Calculate the cumulative sum of the vector.\n",
        "      * Example: `[0.5, 0.3, 0.1, 0.05...]` $\\rightarrow$ `[0.5, 0.8, 0.9, 0.95...]`.\n",
        "3.  **Threshold:** Find the first index where the cumulative sum exceeds $p$ (0.9).\n",
        "4.  **Mask:**\n",
        "      * Keep everything *before* that index.\n",
        "      * Set everything *after* that index to $-\\infty$ (or 0 probability).\n",
        "5.  **Renormalize:** Rescale the remaining chunk so it sums to 1.0.\n",
        "\n",
        "###  Advantage\n",
        "\n",
        "This adapts to the model's confidence (entropy).\n",
        "\n",
        "  * **Low Entropy context (Model is sure):** \"The capital of France is...\"\n",
        "      * \"Paris\" might be 0.99. The cumulative sum hits 0.90 immediately.\n",
        "      * **Result:** The pool size is effectively **1**.\n",
        "  * **High Entropy context (Model is unsure):** \"I want to eat...\"\n",
        "      * \"Pizza\" (0.1), \"Burger\" (0.1), \"Salad\" (0.1)...\n",
        "      * It takes many words to stack up to 0.90.\n",
        "      * **Result:** The pool size dynamically expands to **10 or 20**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "67c03107",
      "metadata": {
        "id": "67c03107"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "class Decoder_Multi_Head_Attention(nn.Module):\n",
        "    def __init__(self, n_head: int, d_model:int, dropout:float=0.0):\n",
        "        super().__init__()\n",
        "        assert d_model % n_head == 0   # should be divisible (d_model % n_head = 0)\n",
        "        self.n_head = n_head\n",
        "        self.d_head = d_model // n_head\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x:torch.Tensor) -> None:\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x).view(B, T, 3, self.n_head, self.d_head)\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout.p if self.training else 0.0, is_causal=True)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "## End Feed Forward of One Block\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, hidden_times: int = 3, dropout: float=0.0) -> None:\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden_times*d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_times*d_model, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.net(x)\n",
        "\n",
        "## one single tranformer block\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model: int, n_head: int, dropout: float=0.2) -> None:\n",
        "        super().__init__()\n",
        "        self.lr1Norm = nn.LayerNorm(d_model)\n",
        "        self.multiHead_attention = Decoder_Multi_Head_Attention(n_head, d_model, dropout)\n",
        "        self.lr2Norm = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x + self.multiHead_attention(self.lr1Norm(x))\n",
        "        x = x + self.ffn(self.lr2Norm(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "## multi block architecture - GPT\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size: int, context_length: int, n_block: int = 4, n_head: int = 4, d_model: int = 256, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.embedings = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.context_length = context_length\n",
        "        self.pos_embedings = nn.Embedding(context_length, d_model)\n",
        "        self.blocks = nn.Sequential(*[Block(d_model, n_head, dropout) for _ in range(n_block)])\n",
        "        self.out_head = nn.Linear(d_model, vocab_size)\n",
        "        self.lrNorm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    # initialising the weights in normal distribution near zero - this results better than random initialising\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std= 0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx: torch.Tensor, targets: Optional[torch.Tensor]=None):\n",
        "        B, T = idx.shape\n",
        "        if T > self.context_length:\n",
        "            idx = idx[:, -self.context_length:]\n",
        "            T = idx.size(1)\n",
        "\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "        x = self.embedings(idx) + self.pos_embedings(pos)\n",
        "        x = self.blocks(x)\n",
        "        x = self.lrNorm(x)\n",
        "        logits = self.out_head(x)\n",
        "        loss = None\n",
        "        if targets is not None :\n",
        "            loss = F.cross_entropy(logits.flatten(0, 1), targets.flatten())\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str = \"\",\n",
        "        max_new_tokens: int = 200,\n",
        "        temperature: float = 1,\n",
        "        top_k: Optional[int] = None,\n",
        "        top_p: Optional[int] = None,\n",
        "        tokenizer=None  # pass your byteTokenizer instance\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate text from a string prompt.\n",
        "        Returns decoded string.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        # Encode prompt\n",
        "\n",
        "        if tokenizer is None:\n",
        "            raise ValueError(\"Tokenizer must be provided to encode prompt.\")\n",
        "        idx = tokenizer.encode(prompt)\n",
        "        idx = torch.tensor([idx], dtype=torch.long, device=next(self.parameters()).device)\n",
        "\n",
        "        # Generate loop\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context if too long\n",
        "            idx_cond = idx if idx.size(1) <= self.context_length else idx[:, -self.context_length:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / max(temperature , 1e-6)\n",
        "\n",
        "            # Optional: top-k sampling\n",
        "            if top_k is not None and top_k > 0:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Optional: top-p (nucleus) sampling\n",
        "            if top_p is not None and top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "            # Sample\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append\n",
        "            idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "\n",
        "        return tokenizer.decode(idx[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5aa7d701",
      "metadata": {
        "id": "5aa7d701"
      },
      "outputs": [],
      "source": [
        "class byteTokenizer:\n",
        "    def __init__(self, file_path: str):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.text = f.read()\n",
        "\n",
        "        # Create sorted list of unique characters\n",
        "        self.chars = sorted(set(self.text))\n",
        "        print(\"Unique characters:\", self.chars)\n",
        "        print(f\"Length of unique chars: {len(self.chars)}\")\n",
        "\n",
        "        # Mapping: char -> int and int -> char\n",
        "        self.char_to_int = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.int_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self)-> int:\n",
        "        return len(self.chars)\n",
        "\n",
        "\n",
        "    def encode(self, s: str) -> list[int]:\n",
        "        \"\"\"Convert string to list of integers\"\"\"\n",
        "        return [self.char_to_int[c] for c in s]\n",
        "\n",
        "    def decode(self, l: list[int]) -> str:\n",
        "        \"\"\"Convert list of integers back to string\"\"\"\n",
        "        return ''.join([self.int_to_char[i] for i in l])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdabe643",
      "metadata": {
        "id": "bdabe643"
      },
      "source": [
        "## Dataset loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e8dd729e",
      "metadata": {
        "id": "e8dd729e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# from tokenizer import byteTokenizer\n",
        "\n",
        "\n",
        "class DatasetLoad(Dataset):\n",
        "    def __init__(self, path: str, context_length: int):\n",
        "        self.tokenizer = byteTokenizer(path)\n",
        "        self.data = torch.tensor(self.tokenizer.encode(self.tokenizer.text), dtype=torch.long)\n",
        "        self.context_length = context_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)-self.context_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx+self.context_length]\n",
        "        y = self.data[idx+1:idx+self.context_length+1]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def create_Dataloader(path: str, batch_size: int, context_length: int, train: float=0.8):\n",
        "    dataset = DatasetLoad(path, context_length)\n",
        "    n = len(dataset)\n",
        "    train_size = int(n * train)\n",
        "    val_size = n - train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    return train_loader, val_loader, dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IkTvhawqTrz5",
      "metadata": {
        "id": "IkTvhawqTrz5"
      },
      "outputs": [],
      "source": [
        "data_path = \"/Data/Ramayan.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242ef847",
      "metadata": {},
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d7832cf",
      "metadata": {},
      "source": [
        "# Training Setup \n",
        "\n",
        "This guide explains how we teach the AI model. Think of training a model like teaching a student: you need a schedule, a good teaching method (optimizer), and ways to be efficient (mixed precision).\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Learning Rate Schedule: How Fast We Learn\n",
        "\n",
        "**The Concept:** The \"Learning Rate\" (LR) is the size of the step the model takes when it learns something new.\n",
        "\n",
        "* **Big Step:** Learns fast, but might miss the correct answer.\n",
        "* **Small Step:** Precise, but takes forever.\n",
        "\n",
        "We use a strategy called **Warmup ‚Üí Cosine Decay ‚Üí Minimum LR**.\n",
        "\n",
        "### How it works (The Journey)\n",
        "\n",
        "1.  **Linear Warmup (The Takeoff):**\n",
        "    * **What happens:** We start with a learning rate of `0` and slowly increase it to the maximum (`peak_lr`).\n",
        "    * **Why:** At the very beginning, the model knows nothing (the weights are random). If we try to learn too fast immediately, the model gets confused and unstable. We start slow to let it \"warm up.\"\n",
        "\n",
        "2.  **Cosine Decay (The Landing):**\n",
        "    * **What happens:** After the warmup, we slowly lower the learning rate following a smooth curve (like a slide).\n",
        "    * **Why:** As the model gets smarter, it needs to make smaller, more careful adjustments to find the perfect answer.\n",
        "\n",
        "3.  **Minimum LR (The Engine Idle):**\n",
        "    * **What happens:** We never let the learning rate hit exactly zero. We stop at a `min_lr`.\n",
        "    * **Why:** We want the model to keep learning slightly until the very last second.\n",
        "\n",
        "\n",
        "\n",
        "### üìå Key Numbers:\n",
        "| Parameter | Value | Simple Explanation |\n",
        "| :--- | :--- | :--- |\n",
        "| `warmup_steps` | 1000 | Take 1000 steps to slowly speed up from 0. |\n",
        "| `peak_lr` | `5e-5` | The top speed. |\n",
        "| `min_lr` | `5e-6` | The slowest speed we allow at the end. |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. The Optimizer: AdamW (The Teacher)\n",
        "\n",
        "We use **AdamW**. This is the algorithm that actually updates the model's brain.\n",
        "\n",
        "### Why \"AdamW\" and not just \"Adam\"?\n",
        "Standard **Adam** is great, but it handles \"Weight Decay\" in a way that can be messy. **AdamW** fixes this.\n",
        "\n",
        "### Key Concepts Simplified:\n",
        "\n",
        "1.  **Betas (`Œ≤1`, `Œ≤2`): The Memory**\n",
        "    * Think of these as the optimizer's \"momentum.\"\n",
        "    * If the optimizer sees the model is improving in a certain direction, **Betas** help it remember that speed and keep going that way.\n",
        "    * **Your values (0.9, 0.95):** These are standard for Large Language Models (LLMs). They help the model react quickly to new information.\n",
        "\n",
        "2.  **Weight Decay: The Cleaner**\n",
        "    * **The Problem:** Sometimes models try to memorize the training data exactly (like a student memorizing answers instead of understanding the topic). This is called \"overfitting.\"\n",
        "    * **The Solution:** Weight decay penalizes the model for having \"weights\" that are too large or complex. It forces the model to keep things simple.\n",
        "    * **Why AdamW is better:** In AdamW, this cleaning process is separate from the learning rate. This means the cleaning stays consistent even when the learning rate slows down.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Mixed Precision Training (Detailed Explanation)\n",
        "\n",
        "This technique allows us to train larger models faster without buying more expensive computers.\n",
        "\n",
        "### The Problem: Precision vs. Speed\n",
        "Computers usually store numbers in **FP32** (32-bit Floating Point).\n",
        "* **FP32:** Very precise (e.g., `0.123456789`), but takes up a lot of memory and is slower to calculate.\n",
        "* **FP16:** Less precise (e.g., `0.1234`), but takes half the memory and is super fast.\n",
        "\n",
        "### The Solution: Mixed Precision\n",
        "We use **both** formats to get the best of both worlds.\n",
        "\n",
        "\n",
        "\n",
        "### Step-by-Step Process:\n",
        "\n",
        "1.  **Master Weights (FP32):**\n",
        "    We keep a \"Master Copy\" of the model in high quality (FP32). This is our safety net.\n",
        "2.  **Forward Pass (FP16):**\n",
        "    When the model reads data and makes a guess, we convert the weights to the fast, lower quality format (FP16). This makes the calculation lightning fast.\n",
        "3.  **Backward Pass (FP16):**\n",
        "    When we calculate the errors (gradients), we also do this in FP16 to stay fast.\n",
        "4.  **The Update (FP32):**\n",
        "    We take those small error calculations and update the **Master Copy (FP32)**.\n",
        "    * *Why?* Because the updates are often very tiny numbers. If we tried to update an FP16 model with a tiny number, the computer might think the number is zero and do nothing. The FP32 Master Copy is sensitive enough to catch these tiny changes.\n",
        "\n",
        "### The \"Loss Scaler\" (Important!)\n",
        "Because FP16 cannot handle very small numbers (they turn to zero), we use a trick called **Scaling**.\n",
        "1.  **Multiply:** Before we calculate the error, we multiply the Loss by a huge number (e.g., 65,000). This makes the small numbers big enough for FP16 to see.\n",
        "2.  **Calculate:** We do the math.\n",
        "3.  **Divide:** Before updating the Master Weights, we divide by that same huge number to return the values to normal size.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Gradient Clipping (Detailed Explanation)\n",
        "\n",
        "You asked about `torch.nn.utils.clip_grad_norm_`. This is a safety mechanism for training stability.\n",
        "\n",
        "### The Analogy: Walking Down a Hill\n",
        "Imagine training is like walking down a steep mountain to find the bottom (the lowest loss).\n",
        "* Usually, you take reasonable steps.\n",
        "* Sometimes, you encounter a \"bad\" piece of data that tells you to take a **massive jump**.\n",
        "* If you take that jump, you might fly off the map and ruin everything. This is called an **Exploding Gradient**.\n",
        "\n",
        "### How Clipping Works\n",
        "**Gradient Clipping** puts a speed limit on your steps.\n",
        "\n",
        "1.  **Calculate Norm:** The computer looks at the total size (magnitude) of all the proposed updates (gradients) for the whole model.\n",
        "2.  **Check Limit:** You set a limit (e.g., `1.0`).\n",
        "3.  **Clip (Cut):**\n",
        "    * If the total size is **0.8**, nothing happens (it's under the limit).\n",
        "    * If the total size is **5.0** (too big!), the computer shrinks the step down until the size is exactly **1.0**.\n",
        "\n",
        "\n",
        "\n",
        "> **Crucial Note:** It shrinks the *size* of the step, but it keeps the *direction* exactly the same. You still go the right way, just with a safer, smaller step.\n",
        "\n",
        "### Why use it?\n",
        "* It prevents the training loss from suddenly spiking to `NaN` (Not a Number) or Infinity.\n",
        "* It allows you to use higher learning rates without crashing the model.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Checklist\n",
        "\n",
        "| Feature | What it is | Why we use it |\n",
        "| :--- | :--- | :--- |\n",
        "| **Cosine Decay** | Slows down learning smoothly | Helps land on the best solution. |\n",
        "| **AdamW** | The \"brain\" updater | Handles model cleanup (weight decay) perfectly. |\n",
        "| **Mixed Precision** | Using FP16 + FP32 | **2x Faster** training, uses **40% less memory**. |\n",
        "| **Grad Scaler** | Multiplies small numbers | Prevents errors from vanishing in FP16. |\n",
        "| **Gradient Clipping** | A speed limit for updates | Prevents the model from crashing due to bad data. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a07a6cc",
      "metadata": {
        "id": "6a07a6cc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_PATH = data_path\n",
        "CONTEXT_LENGTH = 512\n",
        "BATCH_SIZE = 64\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Training settings\n",
        "PRECISION = \"float16\"\n",
        "MAX_ITERS = 1000\n",
        "EVAL_ITERS = 50\n",
        "LOG_INTERVAL = 10\n",
        "GRAD_CLIP = 1.0\n",
        "LEARNING_RATE = 6e-3\n",
        "WARMUP_ITERS = 150\n",
        "MIN_LR = 6e-5\n",
        "WEIGHT_DECAY = 0.1\n",
        "\n",
        "# Model config\n",
        "N_LAYER = 4\n",
        "N_HEAD = 6\n",
        "D_MODEL = 252\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# runs/models path\n",
        "CKPT_DIR = Path(\"/content/runs\")\n",
        "CKPT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def get_lr(it: int):\n",
        "    if it < WARMUP_ITERS:\n",
        "        return LEARNING_RATE * (it + 1) / WARMUP_ITERS\n",
        "    if it > MAX_ITERS:\n",
        "        return MIN_LR\n",
        "    decay_ratio = (it - WARMUP_ITERS) / (MAX_ITERS - WARMUP_ITERS)\n",
        "    coeff = 0.5 * (1.0 + torch.cos(torch.pi * torch.tensor(decay_ratio)))\n",
        "    return MIN_LR + coeff * (LEARNING_RATE - MIN_LR)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, train_loader, val_loader, device, eval_iters=EVAL_ITERS):\n",
        "    model.eval()\n",
        "    losses = {}\n",
        "    for split, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        total_loss = 0.0\n",
        "        for _ in range(eval_iters):\n",
        "            x, y = next(iter(loader))\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=getattr(torch, PRECISION)):\n",
        "                _, loss = model(x, y)\n",
        "            total_loss += loss.item()\n",
        "        losses[split] = total_loss / eval_iters\n",
        "    model.train()\n",
        "    return losses\n",
        "\n",
        "\n",
        "def train():\n",
        "    print(f\"Using device: {DEVICE} | Precision: {PRECISION}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader, val_loader, dataset = create_Dataloader(\n",
        "        path=DATA_PATH,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        context_length=CONTEXT_LENGTH,\n",
        "        train=0.9  # 90% train, 10% val\n",
        "    )\n",
        "\n",
        "    # Get vocab size from tokenizer (we need to extract it)\n",
        "    vocab_size = dataset.tokenizer.vocab_size\n",
        "    print(f\"Vocab size: {vocab_size:,}\")\n",
        "    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = TinyGPT(\n",
        "        vocab_size=vocab_size,\n",
        "        context_length=CONTEXT_LENGTH,\n",
        "        n_block=N_LAYER,\n",
        "        n_head=N_HEAD,\n",
        "        d_model=D_MODEL,\n",
        "        dropout=DROPOUT\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Compile the model for better training \n",
        "    if hasattr(torch, \"compile\"):\n",
        "        print(\"Compiling model..\")\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(0.9, 0.95),\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    scaler = torch.GradScaler(enabled=(PRECISION == \"float16\"))\n",
        "\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\") # you can change this to billion if you have more bigger model, if you want. 1e9:.2f}B\n",
        "    print(\"Starting training...\\n\")\n",
        "\n",
        "    iter_num = 0\n",
        "    best_val_loss = float(\"inf\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    data_iter = iter(train_loader)\n",
        "\n",
        "    pbar = tqdm(range(MAX_ITERS), desc=\"Training\")\n",
        "    for iter_num in pbar:\n",
        "        # Learning rate schedule\n",
        "        lr = get_lr(iter_num)\n",
        "        for g in optimizer.param_groups:\n",
        "            g[\"lr\"] = lr\n",
        "\n",
        "        try:\n",
        "            x, y = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(train_loader)\n",
        "            x, y = next(data_iter)\n",
        "\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        # Forward + backward with AMP\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=getattr(torch, PRECISION)):\n",
        "            _, loss = model(x, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if GRAD_CLIP > 0.0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Logging\n",
        "        if iter_num % LOG_INTERVAL == 0:\n",
        "            tokens_per_sec = (BATCH_SIZE * CONTEXT_LENGTH * LOG_INTERVAL) / (time.time() - start_time + 1e-8)\n",
        "            start_time = time.time()\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{loss.item():.4f}\",\n",
        "                \"lr\": f\"{lr:.2e}\",\n",
        "                \"tok/s\": f\"{tokens_per_sec:.0f}\"\n",
        "            })\n",
        "\n",
        "        # Evaluation\n",
        "        if iter_num % EVAL_ITERS == 0 or iter_num == MAX_ITERS - 1:\n",
        "            losses = estimate_loss(model, train_loader, val_loader, DEVICE)\n",
        "            print(f\"\\nStep {iter_num:,} | \"\n",
        "                  f\"Train: {losses['train']:.4f} | \"\n",
        "                  f\"Val: {losses['val']:.4f} | \"\n",
        "                  f\"LR: {lr:.2e}\")\n",
        "\n",
        "            # Save best model\n",
        "            if losses[\"val\"] < best_val_loss:\n",
        "                best_val_loss = losses[\"val\"]\n",
        "                torch.save({\n",
        "                    \"iter\": iter_num,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"scaler_state_dict\": scaler.state_dict(),\n",
        "                    \"val_loss\": best_val_loss,\n",
        "                    \"config\": {\n",
        "                        \"vocab_size\": vocab_size,\n",
        "                        \"context_length\": CONTEXT_LENGTH,\n",
        "                        \"n_block\": N_LAYER,\n",
        "                        \"n_head\": N_HEAD,\n",
        "                        \"d_model\": D_MODEL,\n",
        "                    }\n",
        "                }, CKPT_DIR / \"best_model.pt\")\n",
        "                print(\"New best model saved!\")\n",
        "\n",
        "        # Periodic checkpoint\n",
        "        if iter_num % 200 == 0 and iter_num > 0:\n",
        "            torch.save(model.state_dict(), CKPT_DIR / f\"checkpoint_iter{iter_num}.pt\")\n",
        "\n",
        "    print(f\"\\nTraining finished! Best validation loss: {best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "LGnYt4RZVHmK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGnYt4RZVHmK",
        "outputId": "66ea44f4-e2bd-48b3-952a-6d93c950e930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda | Precision: float16\n",
            "Unique characters: ['\\n', ' ', '!', '\"', '&', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '√Å', '√Ü', '√â', '√ö', '√ú', '√†', '√°', '√¢', '√¶', '√ß', '√®', '√©', '√™', '√´', '√¨', '√≠', '√Æ', '√Ø', '√±', '√≤', '√¥', '√∂', '√π', '√∫', '√ª', '√º', '≈Ñ', '≈í', '≈ì', '≈ö', '≈õ', '«π', 'Œë', 'Œö', 'Œü', 'Œ†', 'Œ£', 'Œ¨', 'Œ≠', 'ŒÆ', 'ŒØ', 'Œ±', 'Œ≤', 'Œ≥', 'Œ¥', 'Œµ', 'Œ∑', 'Œ∏', 'Œπ', 'Œ∫', 'Œª', 'Œº', 'ŒΩ', 'Œæ', 'Œø', 'œÄ', 'œÅ', 'œÇ', 'œÉ', 'œÑ', 'œÖ', 'œÜ', 'œá', 'œâ', 'œå', 'œç', 'œé', '◊î', '◊ï', '◊ô', '◊¢', '◊¶', '·∏ç', '·πÖ', '·πá', '·πõ', '·π£', '·π≠', '·ºÄ', '·ºê', '·ºò', '·ºù', '·º°', '·º§', '·º∞', '·ºµ', '·º∏', '·ºº', '·ºΩ', '·ΩÅ', '·Ωâ', '·Ωç', '·Ωê', '·Ωë', '·Ω∞', '·Ω¥', '·Ω∫', '·æ∂', '·ø∂', '‚Äî', '‚Äò', '‚Äô', '‚Äú', '‚Äù', '‚Ä¶', '\\ufeff']\n",
            "Length of unique chars: 188\n",
            "Vocab size: 188\n",
            "Train batches: 31591, Val batches: 3510\n",
            "Compiling model with torch.compile()...\n",
            "Total parameters: 2.77M\n",
            "Starting training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 1/1000 [01:34<26:06:47, 94.10s/it, loss=5.2445, lr=4.00e-05, tok/s=6218]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0 | Train: 4.9827 | Val: 4.9908 | LR: 4.00e-05\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   5%|‚ñå         | 51/1000 [01:57<1:13:21,  4.64s/it, loss=2.5092, lr=2.04e-03, tok/s=205838]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 50 | Train: 2.5061 | Val: 2.5121 | LR: 2.04e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  10%|‚ñà         | 101/1000 [02:20<1:10:34,  4.71s/it, loss=2.4653, lr=4.04e-03, tok/s=200616]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 100 | Train: 2.4526 | Val: 2.4492 | LR: 4.04e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  15%|‚ñà‚ñå        | 151/1000 [02:44<1:07:29,  4.77s/it, loss=2.4336, lr=6.00e-03, tok/s=190642]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 150 | Train: 2.4407 | Val: 2.4377 | LR: 6.00e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  20%|‚ñà‚ñà        | 201/1000 [03:07<1:00:32,  4.55s/it, loss=2.4547, lr=5.95e-03, tok/s=188140]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 200 | Train: 2.4303 | Val: 2.4293 | LR: 5.95e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  25%|‚ñà‚ñà‚ñå       | 251/1000 [03:31<59:56,  4.80s/it, loss=2.3348, lr=5.80e-03, tok/s=194823]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 250 | Train: 2.3357 | Val: 2.3391 | LR: 5.80e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  30%|‚ñà‚ñà‚ñà       | 301/1000 [03:54<54:31,  4.68s/it, loss=2.0633, lr=5.56e-03, tok/s=193530]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 300 | Train: 2.0031 | Val: 2.0011 | LR: 5.56e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  35%|‚ñà‚ñà‚ñà‚ñå      | 351/1000 [04:18<51:50,  4.79s/it, loss=1.8022, lr=5.22e-03, tok/s=187446]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 350 | Train: 1.7648 | Val: 1.7604 | LR: 5.22e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 401/1000 [04:43<47:40,  4.78s/it, loss=1.6874, lr=4.82e-03, tok/s=180062]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 400 | Train: 1.6280 | Val: 1.6223 | LR: 4.82e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 451/1000 [05:07<42:49,  4.68s/it, loss=1.6538, lr=4.35e-03, tok/s=180108]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 450 | Train: 1.5553 | Val: 1.5512 | LR: 4.35e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 501/1000 [05:31<39:05,  4.70s/it, loss=1.5839, lr=3.84e-03, tok/s=179559]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 500 | Train: 1.5000 | Val: 1.4925 | LR: 3.84e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 551/1000 [05:56<36:18,  4.85s/it, loss=1.5503, lr=3.30e-03, tok/s=177843]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 550 | Train: 1.4554 | Val: 1.4531 | LR: 3.30e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 601/1000 [06:21<31:56,  4.80s/it, loss=1.4794, lr=2.76e-03, tok/s=174017]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 600 | Train: 1.4202 | Val: 1.4134 | LR: 2.76e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 651/1000 [06:45<27:52,  4.79s/it, loss=1.4380, lr=2.22e-03, tok/s=179989]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 650 | Train: 1.3884 | Val: 1.3792 | LR: 2.22e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 701/1000 [07:10<24:09,  4.85s/it, loss=1.4003, lr=1.71e-03, tok/s=173588]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 700 | Train: 1.3610 | Val: 1.3529 | LR: 1.71e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 751/1000 [07:34<19:30,  4.70s/it, loss=1.3768, lr=1.24e-03, tok/s=177902]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 750 | Train: 1.3374 | Val: 1.3294 | LR: 1.24e-03\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 801/1000 [07:59<16:01,  4.83s/it, loss=1.3567, lr=8.35e-04, tok/s=180850]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 800 | Train: 1.3229 | Val: 1.3052 | LR: 8.35e-04\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 851/1000 [08:24<11:54,  4.79s/it, loss=1.3556, lr=5.05e-04, tok/s=166902]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 850 | Train: 1.3030 | Val: 1.2915 | LR: 5.05e-04\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 901/1000 [08:48<07:44,  4.69s/it, loss=1.3818, lr=2.61e-04, tok/s=171934]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 900 | Train: 1.2902 | Val: 1.2814 | LR: 2.61e-04\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 951/1000 [09:13<03:53,  4.76s/it, loss=1.3219, lr=1.11e-04, tok/s=179957]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 950 | Train: 1.2815 | Val: 1.2744 | LR: 1.11e-04\n",
            "New best model saved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [09:37<00:00,  1.73it/s, loss=1.3496, lr=6.20e-05, tok/s=181518]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 999 | Train: 1.2739 | Val: 1.2712 | LR: 6.00e-05\n",
            "New best model saved!\n",
            "\n",
            "Training finished! Best validation loss: 1.2712\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a27ce8c",
      "metadata": {},
      "source": [
        "# Generation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iiJ3EbtJc9pI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiJ3EbtJc9pI",
        "outputId": "5e211b7e-cf48-4cac-c39d-79690a372a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: ['\\n', ' ', '!', '\"', '&', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '√Å', '√Ü', '√â', '√ö', '√ú', '√†', '√°', '√¢', '√¶', '√ß', '√®', '√©', '√™', '√´', '√¨', '√≠', '√Æ', '√Ø', '√±', '√≤', '√¥', '√∂', '√π', '√∫', '√ª', '√º', '≈Ñ', '≈í', '≈ì', '≈ö', '≈õ', '«π', 'Œë', 'Œö', 'Œü', 'Œ†', 'Œ£', 'Œ¨', 'Œ≠', 'ŒÆ', 'ŒØ', 'Œ±', 'Œ≤', 'Œ≥', 'Œ¥', 'Œµ', 'Œ∑', 'Œ∏', 'Œπ', 'Œ∫', 'Œª', 'Œº', 'ŒΩ', 'Œæ', 'Œø', 'œÄ', 'œÅ', 'œÇ', 'œÉ', 'œÑ', 'œÖ', 'œÜ', 'œá', 'œâ', 'œå', 'œç', 'œé', '◊î', '◊ï', '◊ô', '◊¢', '◊¶', '·∏ç', '·πÖ', '·πá', '·πõ', '·π£', '·π≠', '·ºÄ', '·ºê', '·ºò', '·ºù', '·º°', '·º§', '·º∞', '·ºµ', '·º∏', '·ºº', '·ºΩ', '·ΩÅ', '·Ωâ', '·Ωç', '·Ωê', '·Ωë', '·Ω∞', '·Ω¥', '·Ω∫', '·æ∂', '·ø∂', '‚Äî', '‚Äò', '‚Äô', '‚Äú', '‚Äù', '‚Ä¶', '\\ufeff']\n",
            "Length of unique chars: 188\n",
            "\n",
            "--- Streaming Generation (Temp: 0.8) ---\n",
            "\n",
            "Prompt: rame the mighty arm maid,\n",
            "And leaguestic reverend in hands they grow\n",
            "He gazing and gampion soft and splend.\n",
            "He seat hermit‚Äôs women‚Äôs sweet\n",
            "And eacherous crowd in bright their stored.\n",
            "And strong mad met he dready breast,\n",
            "Straight he prince she saw austing woke\n",
            "Against the billowed, and bank\n",
            "Of swiftly frant his stranged autumn died,\n",
            "And troop the truth them through\n",
            "Stand notten the fast had honour bent and sank,\n",
            "And on and from had to slumbered flew.\n",
            "Nor brother from of his chief and from now,\n",
            "And with laws as a loud to bright.\n",
            "They good with reach penance drew,\n",
            "A like the prince with giants sands\n",
            "They trees and see attented they spired\n",
            "The saint Vi≈õv√°mitra prince and brave,\n",
            "Sugr√≠va by his might to the head\n",
            "Bough fire the humbly rites be fully guard\n",
            "And in the following flashing spring shed.\n",
            "Their flashings, whom the shafts their way.\n",
            "The highest of steed the might,\n",
            "And far the lotus shakens grief to see,\n",
            "Showed proud heavenly friends bent\n",
            "Earth then the A≈õoka three head,\n",
            "Fair the great longed agained in war,\n",
            "His faithful sat on his earth and freed.\n",
            "By slaught all the field his sped,\n",
            "With fire reverent courtest lake\n",
            "The scannot a fell to they paid.\n",
            "He ne‚Äôer his eyes who arms could rest,\n",
            "And was the right wild with their care,\n",
            "Upon his deeds, to the guardiant moon:\n",
            "‚ÄúLike a many a stream,\n",
            "And he free  O King, son realm bried,\n",
            "And blood and palm a blinded ground.\n",
            "For the would the saint the dead.\n",
            "Then her limbs that dear to subdued,\n",
            "The sage who stranquiverse will bows\n",
            "Was brother‚Äôs cruel of heaven away.\n",
            "His serve the woods awful more and king.\n",
            "Or hope these, and all, see much,\n",
            "Each to seen, all who dread display;\n",
            "This with children lord and still know\n",
            "And his high the darest law decree:\n",
            "But R√°va·πá‚Äôs can I go fancy will done\n",
            "The no royal serve to slew.\n",
            "This sacred for the souls and he\n",
            "The lack was the sons to her, the flower,\n",
            "And beauties and the assion cry\n",
            "And from the stand the giant speech.\n",
            "Here faithful ne‚Äôer uncarce and tried:\n",
            "‚ÄúNow stand many a he chiefs of fled\n",
            "To see\n",
            "\n",
            "--- End of Generation ---\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_PATH = Path(\"/content/runs/best_model.pt\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Generation Settings\n",
        "PROMPT = \"ram\"\n",
        "MAX_NEW_TOKENS = 2000\n",
        "TEMPERATURE = 0.8\n",
        "TOP_K = 50\n",
        "\n",
        "def load_best_model(ckpt_path, device):\n",
        "    \"\"\"Same loading logic as before to ensure config matches\"\"\"\n",
        "    if not ckpt_path.exists():\n",
        "        raise FileNotFoundError(f\"Checkpoint not found at {ckpt_path}\")\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    config = checkpoint['config']\n",
        "\n",
        "    model = TinyGPT(\n",
        "        vocab_size=config['vocab_size'],\n",
        "        context_length=config['context_length'],\n",
        "        n_block=config['n_block'],\n",
        "        n_head=config['n_head'],\n",
        "        d_model=config['d_model'],\n",
        "        dropout=0.0\n",
        "    )\n",
        "\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def stream_generate(model, tokenizer, prompt, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    \"\"\"\n",
        "    Generates text and prints it to stdout immediately as tokens are created.\n",
        "    \"\"\"\n",
        "    # 1. Encode and setup\n",
        "    idx = tokenizer.encode(prompt)\n",
        "    idx = torch.tensor([idx], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "    print(f\"Prompt: {prompt}\", end=\"\", flush=True)\n",
        "\n",
        "    current_text = tokenizer.decode(idx[0].tolist())\n",
        "    len_printed = len(current_text)\n",
        "\n",
        "    # Generation Loop\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= model.context_length else idx[:, -model.context_length:]\n",
        "\n",
        "        # Forward pass\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
        "\n",
        "        # Top-K Sampling\n",
        "        if top_k is not None and top_k > 0:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "        full_text = tokenizer.decode(idx[0].tolist())\n",
        "        new_text = full_text[len_printed:]\n",
        "\n",
        "        print(new_text, end=\"\", flush=True) \n",
        "        len_printed += len(new_text)\n",
        "\n",
        "    print(\"\\n\\n--- End of Generation ---\")\n",
        "\n",
        "try:\n",
        "    model = load_best_model(CKPT_PATH, DEVICE)\n",
        "    tokenizer = byteTokenizer(data_path)\n",
        "    # Generation\n",
        "    print(f\"\\n--- Streaming Generation (Temp: {TEMPERATURE}) ---\\n\")\n",
        "    stream_generate(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer, # Requires your tokenizer object\n",
        "        prompt=PROMPT,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_k=TOP_K\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Gpt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
