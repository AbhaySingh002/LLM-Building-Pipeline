{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "196e8556",
      "metadata": {
        "id": "196e8556"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fcbfdf0f",
      "metadata": {
        "id": "fcbfdf0f"
      },
      "outputs": [],
      "source": [
        "class RmsNorm(nn.Module):\n",
        "    def __init__(self, dim: int, epsilon=1e-6):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x: (B, T, D)\n",
        "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.epsilon).rsqrt()\n",
        "        return x * rms * self.gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6c0be5a3",
      "metadata": {
        "id": "6c0be5a3"
      },
      "outputs": [],
      "source": [
        "class SwishFFN(nn.Module):\n",
        "    def __init__(self, d_model: int, hidden_times: int = 3, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(d_model * hidden_times)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden_dim, bias=False),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim, d_model, bias=False),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554f5df5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizer import byteTokenizer\n",
        "from utils import create_Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9e0022f9",
      "metadata": {
        "id": "9e0022f9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class KVCache(nn.Module):\n",
        "    def __init__(self, batch_size, max_seq_len, num_heads, head_dim, dtype=torch.float32):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Type Hinting (Helps the IDE/Linter know these are Tensors)\n",
        "        self.k_cache: torch.Tensor\n",
        "        self.v_cache: torch.Tensor\n",
        "        self.cache_pos: torch.Tensor\n",
        "\n",
        "        # Pre-allocate empty cache\n",
        "        self.register_buffer(\"k_cache\", torch.zeros(batch_size, num_heads, max_seq_len, head_dim, dtype=dtype))\n",
        "        self.register_buffer(\"v_cache\", torch.zeros(batch_size, num_heads, max_seq_len, head_dim, dtype=dtype))\n",
        "\n",
        "        # Position can be a scalar tensor since batch usually generates in sync\n",
        "        self.register_buffer(\"cache_pos\", torch.tensor(0, dtype=torch.long))\n",
        "\n",
        "    def update(self, k_new: torch.Tensor, v_new: torch.Tensor):\n",
        "        # k_new shape: (B, H, T_new, D)\n",
        "        seq_len_new = k_new.size(2)\n",
        "\n",
        "        # 2. Get Scalar Integer for slicing\n",
        "        # We must use .item() because we cannot use a Tensor to define a slice range like [pos : pos+n]\n",
        "        pos = self.cache_pos.item()\n",
        "\n",
        "        # Update the cache\n",
        "        self.k_cache[:, :, pos : pos + seq_len_new, :] = k_new\n",
        "        self.v_cache[:, :, pos : pos + seq_len_new, :] = v_new\n",
        "\n",
        "        # Update position\n",
        "        self.cache_pos += seq_len_new\n",
        "\n",
        "        # 3. CRITICAL: Return only the valid data\n",
        "        # If we return the full buffer, Attention will see the empty zeros and mess up results.\n",
        "        # We slice up to the current filled length.\n",
        "        current_len = pos + seq_len_new\n",
        "        return (\n",
        "            self.k_cache[:, :, :current_len, :],\n",
        "            self.v_cache[:, :, :current_len, :]\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.cache_pos.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6f833b54",
      "metadata": {
        "id": "6f833b54"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional\n",
        "\n",
        "# [Keep your imports, RmsNorm, SwishFFN, and KVCache classes as they are]\n",
        "\n",
        "class Decoder_Multi_Head_Attention(nn.Module):\n",
        "    def __init__(self, n_head: int, d_model: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        assert d_model % n_head == 0, \"d_model must be divisible by n_head\"\n",
        "        self.n_head = n_head\n",
        "        self.d_head = d_model // n_head\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, is_inference: bool = False, kv_cache: Optional['KVCache'] = None) -> torch.Tensor:\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x).view(B, T, 3, self.n_head, self.d_head)\n",
        "\n",
        "        # Split q, k, v\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "\n",
        "        # Transpose for attention: (B, n_head, T, d_head)\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # KV Cache Logic\n",
        "        if is_inference and kv_cache is not None:\n",
        "            # Update cache with new k, v and get the full history back\n",
        "            k, v = kv_cache.update(k, v)\n",
        "\n",
        "        # Scaled Dot Product Attention\n",
        "        # CRITICAL FIX: During inference decoding (T=1), we attend to ALL past keys (k),\n",
        "        # so is_causal must be False. During training or prefill, it is True.\n",
        "        use_causal_mask = not is_inference or (is_inference and T > 1)\n",
        "\n",
        "        # If we are in inference and T=1 (decoding), we don't apply dropout\n",
        "        dropout_p = self.dropout.p if self.training else 0.0\n",
        "\n",
        "        out = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=None,\n",
        "            dropout_p=dropout_p,\n",
        "            is_causal=use_causal_mask\n",
        "        )\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model: int, n_head: int, dropout: float = 0.2) -> None:\n",
        "        super().__init__()\n",
        "        self.lr1Norm = RmsNorm(d_model)\n",
        "        self.multiHead_attention = Decoder_Multi_Head_Attention(n_head, d_model, dropout)\n",
        "        self.lr2Norm = RmsNorm(d_model)\n",
        "        self.ffn = SwishFFN(d_model, 4, dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, is_inference: bool = False, kv_cache: Optional['KVCache'] = None):\n",
        "        # Pass inference flags down to attention\n",
        "        x = x + self.multiHead_attention(self.lr1Norm(x), is_inference=is_inference, kv_cache=kv_cache)\n",
        "        x = x + self.ffn(self.lr2Norm(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2c5d279e",
      "metadata": {
        "id": "2c5d279e"
      },
      "outputs": [],
      "source": [
        "class TinyGPT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        context_length: int,\n",
        "        n_block: int = 4,\n",
        "        n_head: int = 4,\n",
        "        d_model: int = 256,\n",
        "        dropout: float = 0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedings = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.context_length = context_length\n",
        "        self.n_head = n_head\n",
        "\n",
        "        self.pos_embedings = nn.Embedding(context_length, d_model)\n",
        "        self.blocks = nn.ModuleList([Block(d_model, n_head, dropout) for _ in range(n_block)]) # Changed to ModuleList to iterate easily\n",
        "        self.out_head = nn.Linear(d_model, vocab_size)\n",
        "        self.lrNorm = nn.LayerNorm(d_model) # Or RmsNorm if you prefer consistency\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        idx: torch.Tensor,\n",
        "        targets: Optional[torch.Tensor] = None,\n",
        "        kv_cache: Optional['KVCache'] = None,\n",
        "        is_inference: bool = False,\n",
        "        start_pos: int = 0\n",
        "    ):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # During training/prefill, crop if needed. During decoding, we assume 1 token.\n",
        "        if T > self.context_length:\n",
        "            idx = idx[:, -self.context_length:]\n",
        "            T = idx.size(1)\n",
        "\n",
        "        # Create position indices based on start_pos\n",
        "        pos = torch.arange(start_pos, start_pos + T, device=idx.device).unsqueeze(0) # (1, T)\n",
        "\n",
        "        x = self.embedings(idx) + self.pos_embedings(pos)\n",
        "\n",
        "        # Pass args down to blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, is_inference=is_inference, kv_cache=kv_cache)\n",
        "\n",
        "        x = self.lrNorm(x)\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.flatten(0, 1), targets.flatten())\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        tokenizer,\n",
        "        max_new_tokens: int = 200,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: Optional[int] = None,\n",
        "        stream: bool = False\n",
        "    ):\n",
        "        self.eval()\n",
        "        idx = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long, device=next(self.parameters()).device)\n",
        "\n",
        "        # 1. Initialize KV Cache\n",
        "        B = idx.shape[0]\n",
        "        kv_cache = KVCache(B, self.context_length, self.n_head, self.d_model // self.n_head, dtype=next(self.parameters()).dtype).to(idx.device)\n",
        "        kv_cache.reset()\n",
        "\n",
        "        # 2. Prefill Phase (Process the prompt)\n",
        "        # We pass the whole prompt. T > 1, so attention acts causally.\n",
        "        logits, _ = self(idx, is_inference=True, kv_cache=kv_cache, start_pos=0)\n",
        "\n",
        "        # Get the last token's logits to predict the first new token\n",
        "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
        "\n",
        "        # Sampling logic (extract to helper if needed)\n",
        "        probs = self._sample_logits(logits, top_k)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # If not streaming, we collect tokens here\n",
        "        generated_tokens = [next_token.item()]\n",
        "        if stream:\n",
        "            yield next_token.item()\n",
        "\n",
        "        # 3. Decoding Phase (Token by Token)\n",
        "        # Input is now just (B, 1). kv_cache remembers the past.\n",
        "        input_token = next_token\n",
        "\n",
        "        for i in range(max_new_tokens - 1):\n",
        "            # Calculate current position in sequence (prompt len + generated so far)\n",
        "            current_pos = idx.shape[1] + i\n",
        "\n",
        "            # Stop if context limit reached\n",
        "            if current_pos >= self.context_length:\n",
        "                break\n",
        "\n",
        "            # Forward pass with ONLY the new token\n",
        "            logits, _ = self(input_token, is_inference=True, kv_cache=kv_cache, start_pos=current_pos)\n",
        "\n",
        "            logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
        "            probs = self._sample_logits(logits, top_k)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            input_token = next_token # Update input for next iteration\n",
        "            generated_tokens.append(next_token.item())\n",
        "\n",
        "            if stream:\n",
        "                yield next_token.item()\n",
        "\n",
        "        if not stream:\n",
        "            # Combine prompt + generated for full output\n",
        "            full_idx = idx.tolist()[0] + generated_tokens\n",
        "            return tokenizer.decode(full_idx)\n",
        "\n",
        "    def _sample_logits(self, logits, top_k):\n",
        "        if top_k is not None and top_k > 0:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "        return torch.softmax(logits, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d8eb4ef9",
      "metadata": {
        "id": "d8eb4ef9"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/Ramayan.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "914d4cfb",
      "metadata": {
        "id": "914d4cfb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_PATH = data_path\n",
        "CONTEXT_LENGTH = 512\n",
        "BATCH_SIZE = 64\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Training settings\n",
        "PRECISION = \"float16\"\n",
        "MAX_ITERS = 1000\n",
        "EVAL_ITERS = 50\n",
        "LOG_INTERVAL = 10\n",
        "GRAD_CLIP = 1.0\n",
        "LEARNING_RATE = 6e-3\n",
        "WARMUP_ITERS = 150\n",
        "MIN_LR = 8e-5\n",
        "WEIGHT_DECAY = 0.1\n",
        "\n",
        "# Model config\n",
        "N_LAYER = 4\n",
        "N_HEAD = 6\n",
        "D_MODEL = 252\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# runs/models path\n",
        "CKPT_DIR = Path(\"/content/runs\")\n",
        "CKPT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def get_lr(it: int):\n",
        "    if it < WARMUP_ITERS:\n",
        "        return LEARNING_RATE * (it + 1) / WARMUP_ITERS\n",
        "    if it > MAX_ITERS:\n",
        "        return MIN_LR\n",
        "    decay_ratio = (it - WARMUP_ITERS) / (MAX_ITERS - WARMUP_ITERS)\n",
        "    coeff = 0.5 * (1.0 + torch.cos(torch.pi * torch.tensor(decay_ratio)))\n",
        "    return MIN_LR + coeff * (LEARNING_RATE - MIN_LR)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, train_loader, val_loader, device, eval_iters=EVAL_ITERS):\n",
        "    model.eval()\n",
        "    losses = {}\n",
        "    for split, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        total_loss = 0.0\n",
        "        for _ in range(eval_iters):\n",
        "            x, y = next(iter(loader))\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=getattr(torch, PRECISION)):\n",
        "                _, loss = model(x, y)\n",
        "            total_loss += loss.item()\n",
        "        losses[split] = total_loss / eval_iters\n",
        "    model.train()\n",
        "    return losses\n",
        "\n",
        "\n",
        "def train():\n",
        "    print(f\"Using device: {DEVICE} | Precision: {PRECISION}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader, val_loader, dataset = create_Dataloader(\n",
        "        path=DATA_PATH,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        context_length=CONTEXT_LENGTH,\n",
        "        train=0.9  # 90% train, 10% val\n",
        "    )\n",
        "\n",
        "    # Get vocab size from tokenizer (we need to extract it)\n",
        "    vocab_size = dataset.tokenizer.vocab_size\n",
        "    print(f\"Vocab size: {vocab_size:,}\")\n",
        "    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = TinyGPT(\n",
        "        vocab_size=vocab_size,\n",
        "        context_length=CONTEXT_LENGTH,\n",
        "        n_block=N_LAYER,\n",
        "        n_head=N_HEAD,\n",
        "        d_model=D_MODEL,\n",
        "        dropout=DROPOUT\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Compile the model for better training\n",
        "    if hasattr(torch, \"compile\"):\n",
        "        print(\"Compiling model..\")\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(0.9, 0.95),\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    scaler = torch.GradScaler(enabled=(PRECISION == \"float16\"))\n",
        "\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\") # you can change this to billion if you have more bigger model, if you want. 1e9:.2f}B\n",
        "    print(\"Starting training...\\n\")\n",
        "\n",
        "    iter_num = 0\n",
        "    best_val_loss = float(\"inf\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    data_iter = iter(train_loader)\n",
        "\n",
        "    pbar = tqdm(range(MAX_ITERS), desc=\"Training\")\n",
        "    for iter_num in pbar:\n",
        "        # Learning rate schedule\n",
        "        lr = get_lr(iter_num)\n",
        "        for g in optimizer.param_groups:\n",
        "            g[\"lr\"] = lr\n",
        "\n",
        "        try:\n",
        "            x, y = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(train_loader)\n",
        "            x, y = next(data_iter)\n",
        "\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        # Forward + backward with AMP\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=getattr(torch, PRECISION)):\n",
        "            _, loss = model(x, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if GRAD_CLIP > 0.0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Logging\n",
        "        if iter_num % LOG_INTERVAL == 0:\n",
        "            tokens_per_sec = (BATCH_SIZE * CONTEXT_LENGTH * LOG_INTERVAL) / (time.time() - start_time + 1e-8)\n",
        "            start_time = time.time()\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{loss.item():.4f}\",\n",
        "                \"lr\": f\"{lr:.2e}\",\n",
        "                \"tok/s\": f\"{tokens_per_sec:.0f}\"\n",
        "            })\n",
        "\n",
        "        # Evaluation\n",
        "        if iter_num % EVAL_ITERS == 0 or iter_num == MAX_ITERS - 1:\n",
        "            losses = estimate_loss(model, train_loader, val_loader, DEVICE)\n",
        "            print(f\"\\nStep {iter_num:,} | \"\n",
        "                  f\"Train: {losses['train']:.4f} | \"\n",
        "                  f\"Val: {losses['val']:.4f} | \"\n",
        "                  f\"LR: {lr:.2e}\")\n",
        "\n",
        "            # Save best model\n",
        "            if losses[\"val\"] < best_val_loss:\n",
        "                best_val_loss = losses[\"val\"]\n",
        "                torch.save({\n",
        "                    \"iter\": iter_num,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"scaler_state_dict\": scaler.state_dict(),\n",
        "                    \"val_loss\": best_val_loss,\n",
        "                    \"config\": {\n",
        "                        \"vocab_size\": vocab_size,\n",
        "                        \"context_length\": CONTEXT_LENGTH,\n",
        "                        \"n_block\": N_LAYER,\n",
        "                        \"n_head\": N_HEAD,\n",
        "                        \"d_model\": D_MODEL,\n",
        "                    }\n",
        "                }, CKPT_DIR / \"best_model.pt\")\n",
        "                print(\"New best model saved!\")\n",
        "\n",
        "        # Periodic checkpoint\n",
        "        if iter_num % 200 == 0 and iter_num > 0:\n",
        "            torch.save(model.state_dict(), CKPT_DIR / f\"checkpoint_iter{iter_num}.pt\")\n",
        "\n",
        "    print(f\"\\nTraining finished! Best validation loss: {best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a3ed0f1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "777c40cf60384af48312606b865982f1",
            "188f4ccef89a46719a2f2f22b18fc615",
            "a01b646fc1d24e339c324a29c2524bcc",
            "e028665e305f48058d6d12004c30b9f0",
            "b97fed3eedae4577922ea4f494037edf",
            "90eaaf960bf043dcbe7a724fcd79223d",
            "313daff4a3614901a0edb319e5d5f044",
            "1872dcf3e8704409b2e21fba14928ace",
            "9815190b58c34c9990ab0b96036f7388",
            "1751b2bbbbea4c2e837419c28e3775b3",
            "cabb985c2b264daab23accc66ee53e50"
          ]
        },
        "id": "a3ed0f1e",
        "outputId": "63b4e4a1-233e-49e8-f5b3-d18c4b1c5a34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda | Precision: float16\n",
            "Unique characters: ['\\n', ' ', '!', '\"', '&', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', 'Á', 'Æ', 'É', 'Ú', 'Ü', 'à', 'á', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ñ', 'ò', 'ô', 'ö', 'ù', 'ú', 'û', 'ü', 'ń', 'Œ', 'œ', 'Ś', 'ś', 'ǹ', 'Α', 'Κ', 'Ο', 'Π', 'Σ', 'ά', 'έ', 'ή', 'ί', 'α', 'β', 'γ', 'δ', 'ε', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ω', 'ό', 'ύ', 'ώ', 'ה', 'ו', 'י', 'ע', 'צ', 'ḍ', 'ṅ', 'ṇ', 'ṛ', 'ṣ', 'ṭ', 'ἀ', 'ἐ', 'Ἐ', 'Ἕ', 'ἡ', 'ἤ', 'ἰ', 'ἵ', 'Ἰ', 'Ἴ', 'Ἵ', 'ὁ', 'Ὁ', 'Ὅ', 'ὐ', 'ὑ', 'ὰ', 'ὴ', 'ὺ', 'ᾶ', 'ῶ', '—', '‘', '’', '“', '”', '…', '\\ufeff']\n",
            "Length of unique chars: 188\n",
            "Vocab size: 188\n",
            "Train batches: 31591, Val batches: 3510\n",
            "Compiling model..\n",
            "Total parameters: 3.27M\n",
            "Starting training...\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "777c40cf60384af48312606b865982f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  return torch._C._get_cublas_allow_tf32()\n",
            "W1124 11:00:35.376000 289 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0 | Train: 5.0009 | Val: 5.0043 | LR: 4.00e-05\n",
            "New best model saved!\n",
            "\n",
            "Step 50 | Train: 2.5208 | Val: 2.5350 | LR: 2.04e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 100 | Train: 2.4681 | Val: 2.4840 | LR: 4.04e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 150 | Train: 2.5007 | Val: 2.5190 | LR: 6.00e-03\n",
            "\n",
            "Step 200 | Train: 2.4248 | Val: 2.4383 | LR: 5.95e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 250 | Train: 2.3818 | Val: 2.3944 | LR: 5.80e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 300 | Train: 2.0139 | Val: 2.0411 | LR: 5.56e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 350 | Train: 1.7902 | Val: 1.8261 | LR: 5.23e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 400 | Train: 1.6421 | Val: 1.6944 | LR: 4.82e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 450 | Train: 1.5699 | Val: 1.6111 | LR: 4.36e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 500 | Train: 1.4955 | Val: 1.5371 | LR: 3.85e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 550 | Train: 1.4494 | Val: 1.4940 | LR: 3.31e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 600 | Train: 1.4107 | Val: 1.4511 | LR: 2.77e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 650 | Train: 1.3824 | Val: 1.4240 | LR: 2.23e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 700 | Train: 1.3543 | Val: 1.3939 | LR: 1.72e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 750 | Train: 1.3251 | Val: 1.3694 | LR: 1.26e-03\n",
            "New best model saved!\n",
            "\n",
            "Step 800 | Train: 1.3081 | Val: 1.3464 | LR: 8.53e-04\n",
            "New best model saved!\n",
            "\n",
            "Step 850 | Train: 1.2922 | Val: 1.3323 | LR: 5.23e-04\n",
            "New best model saved!\n",
            "\n",
            "Step 900 | Train: 1.2826 | Val: 1.3237 | LR: 2.80e-04\n",
            "New best model saved!\n",
            "\n",
            "Step 950 | Train: 1.2701 | Val: 1.3153 | LR: 1.30e-04\n",
            "New best model saved!\n",
            "\n",
            "Step 999 | Train: 1.2742 | Val: 1.3115 | LR: 8.00e-05\n",
            "New best model saved!\n",
            "\n",
            "Training finished! Best validation loss: 1.3115\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fd9dbeec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd9dbeec",
        "outputId": "1be9ec21-a68f-45c0-b050-bdb423e85773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: ['\\n', ' ', '!', '\"', '&', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', 'Á', 'Æ', 'É', 'Ú', 'Ü', 'à', 'á', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ñ', 'ò', 'ô', 'ö', 'ù', 'ú', 'û', 'ü', 'ń', 'Œ', 'œ', 'Ś', 'ś', 'ǹ', 'Α', 'Κ', 'Ο', 'Π', 'Σ', 'ά', 'έ', 'ή', 'ί', 'α', 'β', 'γ', 'δ', 'ε', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ω', 'ό', 'ύ', 'ώ', 'ה', 'ו', 'י', 'ע', 'צ', 'ḍ', 'ṅ', 'ṇ', 'ṛ', 'ṣ', 'ṭ', 'ἀ', 'ἐ', 'Ἐ', 'Ἕ', 'ἡ', 'ἤ', 'ἰ', 'ἵ', 'Ἰ', 'Ἴ', 'Ἵ', 'ὁ', 'Ὁ', 'Ὅ', 'ὐ', 'ὑ', 'ὰ', 'ὴ', 'ὺ', 'ᾶ', 'ῶ', '—', '‘', '’', '“', '”', '…', '\\ufeff']\n",
            "Length of unique chars: 188\n",
            "\n",
            "--- Streaming Generation (Temp: 0.8) ---\n",
            "\n",
            "Prompt: sumitra donarched three alone,\n",
            "Which the haste, the the choice of due to hold?\n",
            "They shall world like the borne into trace\n",
            "That Báli and the chief and due.\n",
            "Now slain their cast this heaven of his passed:\n",
            "I son of the light, and all o’er\n",
            "For their mighty mass, them words of old:\n",
            "The rights of the lake to sea ill.”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Canto XXII. The Sáran Bharat’s Lament.\n",
            "\n",
            "\n",
            "Due spoke their doubtful in they child\n",
            "The hermit in their brother and give they prayer sped\n",
            "And thus their king and king mind,\n",
            "And nor she words her stood the those,\n",
            "So hermit boon of the bafflee,\n",
            "Her as their gave in battle bands.\n",
            "And sweet round the mountain care,\n",
            "And every paid with golden words there,\n",
            "As each her victor who aged and showed,\n",
            "And fearful who from from the spread,\n",
            "Their carest in and raised with and found,\n",
            "And drum on to my restored fell\n",
            "To such in royal rules beside,\n",
            "A monarch son of us in deep that thrown,\n",
            "With virtue to the souther’s race,\n",
            "Such soul his sorrow the sons to die.\n",
            "With the foes the sped him rites\n",
            "As on the saw the blessing in drews.\n",
            "Violen through the Gods and trees,\n",
            "Though all the chasely might and hearts hearts to words he bowers,\n",
            "The fruitless were of braves were came,\n",
            "In car and he shall his shall speech.\n",
            "And moonless around a spake\n",
            "O’er the mighty lordly despair\n",
            "To many a rough of kings of the sky.\n",
            "Stands and a mountain, fall and died,\n",
            "And black the day with blissful filled.\n",
            "And her sad earth men in cheere,\n",
            "Who wished the arms as their way.\n",
            "Fault all the saints the land rage\n",
            "Replied the king in the flood earthly sped\n",
            "The conscious that they plain\n",
            "By Lakshmaṇ and Ráma’s ran and shade,\n",
            "And every doubter disguise there.\n",
            "On the saw the feet mercy,\n",
            "And the wild from each life water they bred.\n",
            "The circling forms her breast and would and sped,\n",
            "And Khara king above and o’er there.\n",
            "Bright they sat the noble roam\n",
            "The other woods of battled arrows.\n",
            "Be their ferve the rivers’ son\n",
            "The people’s deers who deadly dressed\n",
            "To my bosom who hope is heart is feet,\n",
            "Let the chaplains at the skies:\n",
            "And Sí\n",
            "\n",
            "--- End of Generation ---\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_PATH = Path(\"/content/runs/best_model.pt\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Generation Settings\n",
        "PROMPT = \"sumitra\"\n",
        "MAX_NEW_TOKENS = 2000\n",
        "TEMPERATURE = 0.8\n",
        "TOP_K = 50\n",
        "\n",
        "def load_best_model(ckpt_path, device):\n",
        "    \"\"\"Same loading logic as before to ensure config matches\"\"\"\n",
        "    if not ckpt_path.exists():\n",
        "        raise FileNotFoundError(f\"Checkpoint not found at {ckpt_path}\")\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    config = checkpoint['config']\n",
        "\n",
        "    model = TinyGPT(\n",
        "        vocab_size=config['vocab_size'],\n",
        "        context_length=config['context_length'],\n",
        "        n_block=config['n_block'],\n",
        "        n_head=config['n_head'],\n",
        "        d_model=config['d_model'],\n",
        "        dropout=0.0\n",
        "    )\n",
        "\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def stream_generate(model, tokenizer, prompt, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    \"\"\"\n",
        "    Generates text and prints it to stdout immediately as tokens are created.\n",
        "    \"\"\"\n",
        "    # 1. Encode and setup\n",
        "    idx = tokenizer.encode(prompt)\n",
        "    idx = torch.tensor([idx], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "    print(f\"Prompt: {prompt}\", end=\"\", flush=True)\n",
        "\n",
        "    current_text = tokenizer.decode(idx[0].tolist())\n",
        "    len_printed = len(current_text)\n",
        "\n",
        "    # Generation Loop\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= model.context_length else idx[:, -model.context_length:]\n",
        "\n",
        "        # Forward pass\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
        "\n",
        "        # Top-K Sampling\n",
        "        if top_k is not None and top_k > 0:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "        full_text = tokenizer.decode(idx[0].tolist())\n",
        "        new_text = full_text[len_printed:]\n",
        "\n",
        "        print(new_text, end=\"\", flush=True)\n",
        "        len_printed += len(new_text)\n",
        "\n",
        "    print(\"\\n\\n--- End of Generation ---\")\n",
        "\n",
        "try:\n",
        "    model = load_best_model(CKPT_PATH, DEVICE)\n",
        "    tokenizer = byteTokenizer(data_path)\n",
        "    # Generation\n",
        "    print(f\"\\n--- Streaming Generation (Temp: {TEMPERATURE}) ---\\n\")\n",
        "    stream_generate(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer, # Requires your tokenizer object\n",
        "        prompt=PROMPT,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_k=TOP_K\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Gpt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1751b2bbbbea4c2e837419c28e3775b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1872dcf3e8704409b2e21fba14928ace": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "188f4ccef89a46719a2f2f22b18fc615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90eaaf960bf043dcbe7a724fcd79223d",
            "placeholder": "​",
            "style": "IPY_MODEL_313daff4a3614901a0edb319e5d5f044",
            "value": "Training: 100%"
          }
        },
        "313daff4a3614901a0edb319e5d5f044": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "777c40cf60384af48312606b865982f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_188f4ccef89a46719a2f2f22b18fc615",
              "IPY_MODEL_a01b646fc1d24e339c324a29c2524bcc",
              "IPY_MODEL_e028665e305f48058d6d12004c30b9f0"
            ],
            "layout": "IPY_MODEL_b97fed3eedae4577922ea4f494037edf"
          }
        },
        "90eaaf960bf043dcbe7a724fcd79223d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9815190b58c34c9990ab0b96036f7388": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a01b646fc1d24e339c324a29c2524bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1872dcf3e8704409b2e21fba14928ace",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9815190b58c34c9990ab0b96036f7388",
            "value": 1000
          }
        },
        "b97fed3eedae4577922ea4f494037edf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cabb985c2b264daab23accc66ee53e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e028665e305f48058d6d12004c30b9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1751b2bbbbea4c2e837419c28e3775b3",
            "placeholder": "​",
            "style": "IPY_MODEL_cabb985c2b264daab23accc66ee53e50",
            "value": " 1000/1000 [08:22&lt;00:00,  4.56s/it, loss=1.3565, lr=8.20e-05, tok/s=196930]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
