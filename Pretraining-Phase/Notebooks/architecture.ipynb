{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87f8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de235d",
   "metadata": {},
   "outputs": [],
   "source": [
    " ### Sample to just understand the multihead\n",
    "\n",
    "context_length = 5 ## just a sentence length\n",
    "batch_size = 1\n",
    "n_embd = 12\n",
    "n_head = 3\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038181f",
   "metadata": {},
   "source": [
    "### Two Approaches to Multi-Head Attention\n",
    "\n",
    "#### 1. Single Linear Layer (Industry Standard Approach)\n",
    "- In this method, **one Linear layer** generates **Q, K, and V together**.\n",
    "- The layer projects the input from `d_model â†’ 3 Ã— d_model` in a single forward pass.\n",
    "- After that, the output is **reshaped and split into multiple heads**.\n",
    "- This approach is **fast and efficient**, which is why it is used in almost all production-grade Transformer models (e.g., GPT, BERT, etc.).\n",
    "\n",
    "#### 2. Separate Per-Head Linear Layers (Parallel Head Construction)\n",
    "- In this method, **each attention head has its own independent Linear layers** for Q, K, and V.\n",
    "- Heads operate **independently**, and their outputs are **combined at the end**.\n",
    "- This approach is **conceptually simpler**, but **slower** because:\n",
    "  - More linear layers\n",
    "  - More parameter copies\n",
    "  - Less efficient GPU batching\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Why Approach 1 is Preferred\n",
    "\n",
    "1. **Efficiency / Speed**\n",
    "   - One linear layer for QKV means **fewer matrix multiplications** than separate layers.\n",
    "   - Computation is done in a **single large matrix operation**, reducing overhead and improving GPU parallelism.\n",
    "\n",
    "2. **Industry Usage**\n",
    "   - Most production models (e.g., GPT, BERT) use this method because it is **fast and memory-efficient**.\n",
    "   - The concatenated headsâ€™ weights are implemented with a **single linear layer** for simplicity and scalability.\n",
    "\n",
    "3. **Memory Efficiency**\n",
    "   - Fewer linear layers â†’ **fewer parameters** â†’ **less memory usage**, which is critical for large-scale models.\n",
    "\n",
    "---\n",
    "\n",
    "### â— Caveats / When Separate Heads Might Make Sense\n",
    "\n",
    "- Separate per-head layers offer **more flexibility**: each head can learn **very different projections**.\n",
    "- For **small models or teaching purposes**, the performance difference is **minimal**.\n",
    "- **Specialized architectures** or **memory-constrained inference** may use variants like **multi-query attention**.\n",
    "- The single-linear approach **assumes all heads have equal dimension**; if you need **different head sizes**, separate layers are required.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”Ž Summary / Conclusion\n",
    "\n",
    "- âœ… **Single Linear for QKV** is widely used in industry: **faster**, **memory-efficient**, and **scalable**.\n",
    "- âš ï¸ **Separate per-head layers** are **slower** and mostly used for **teaching** or **research experiments**.\n",
    "- While **Approach 1 dominates in practice**, **Approach 2** can be useful for **custom or experimental models**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd856761",
   "metadata": {},
   "source": [
    "## First approach to build the Multi Head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02758669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, n_head: int, d_model: int, context_length: int, dropout: float = 0.0, log_shape: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.log_shape = log_shape\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Each attention head must get the same number of dimensions.\n",
    "        # Example: d_model = 12 and n_head = 3 â†’ d_head = 4\n",
    "        assert d_model % n_head == 0, \"Embedding size must be divisible by number of heads\"\n",
    "        self.d_head = d_model // n_head\n",
    "\n",
    "        # This linear layer produces Q, K, and V together.\n",
    "        # Input:  (B, T, d_model)\n",
    "        # Output: (B, T, 3*d_model)\n",
    "        # Because we need Q, K, V â€” each of size d_model.\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        \n",
    "        # After attention finishes, all heads are merged back\n",
    "        # and passed through this output projection.\n",
    "        self.projection = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask:\n",
    "        # Upper triangular matrix with True above diagonal.\n",
    "        # This prevents a token from looking at future tokens.\n",
    "        self_mask = torch.triu(\n",
    "            torch.ones(context_length, context_length, dtype=torch.bool),\n",
    "            diagonal=1\n",
    "        )\n",
    "        # register_buffer ensures this mask moves with the model (GPU/CPU)\n",
    "        self.register_buffer(\"causal_mask\", self_mask)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        '''\n",
    "        Here we have 3 self-attention heads.  \n",
    "        Each head needs its own Q, K, V vectors, and all of them must have the same size.\n",
    "\n",
    "        1. We pass 1 sentence â†’ it has 5 tokens â†’ each token has a 12-dimensional embedding.\n",
    "        So the input shape is: (1, 5, 12)\n",
    "\n",
    "        2. The qkv() layer is a Linear layer that creates all three matrices:\n",
    "            Q, K, and V\n",
    "        It does this by projecting the input from:\n",
    "            d_model â†’ 3 * d_model\n",
    "        So output shape becomes: (1, 5, 36)\n",
    "\n",
    "        3. Now the 36 channels can be evenly divided into 3 heads.\n",
    "        Because:\n",
    "            d_model = 12\n",
    "            n_heads = 3\n",
    "            head_dim = 12 / 3 = 4\n",
    "\n",
    "        4. Therefore, each head will receive:\n",
    "            Q of size 4,\n",
    "            K of size 4,\n",
    "            V of size 4\n",
    "        and this is repeated for all 3 heads.\n",
    "\n",
    "        In short:\n",
    "        - Input:       (1, 5, 12)\n",
    "        - After qkv(): (1, 5, 36)\n",
    "        - Split into heads: 3 heads Ã— 4 dims each for Q, K, V\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(B, T, 3, self.n_head, self.d_head)\n",
    "\n",
    "        if self.log_shape:\n",
    "            print(f\"Shape of QKV: {qkv.shape}\")\n",
    "\n",
    "        # Step 3: Split into Q, K, V\n",
    "        # Each has shape (B, T, n_head, d_head)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "\n",
    "        '''\n",
    "    bcz  at present -> (B, T, n_head, d_head). and the multihead attention wants the (B, n_head, T, d_head)\n",
    "    \n",
    "    q = (B, heads, T, d_head)\n",
    "    k = (B, heads, T, d_head)\n",
    "    v = (B, heads, T, d_head)\n",
    "    \n",
    "    Each attention head now has access to all T tokens, each represented as a d_head-dim vector.\n",
    "    \n",
    "    '''\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        if self.log_shape:\n",
    "            print(\"q:\", q.shape, \"k:\", k.shape, \"v:\", v.shape)\n",
    "\n",
    "        # scale factor = 1 / sqrt(d_head)\n",
    "        scale = 1.0 / math.sqrt(self.d_head)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        k.transpose(-2, -1):  (B, heads, d_head, T)\n",
    "    \n",
    "        (B,heads,T,d_head) \n",
    "            @\n",
    "        (B,heads,d_head,T)\n",
    "        --------------------------------\n",
    "        â†’ (B, heads, T, T)\n",
    "        \n",
    "        attention score matrix of T*T :->  each head computes a 5*5 matrix \n",
    "            scores[i,j] = how much token i attends to token j\n",
    "            \n",
    "        scake = 1 / sqrt(d_head)   ->  This avoids very large dot-products which would make softmax explode\n",
    "        \n",
    "        '''\n",
    "        attention_weight = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "\n",
    "        \n",
    "        #  token positions by setting them to -inf\n",
    "        attention_weight = attention_weight.masked_fill(\n",
    "            self.causal_mask[:T, :T], \n",
    "            float('-inf')\n",
    "        )\n",
    "\n",
    "        #  Convert scores to probabilities\n",
    "        # Softmax applied on last dim so each row sums to 1\n",
    "        attention_weight_prob = F.softmax(attention_weight, dim=-1)\n",
    "        attention_weight_prob = self.dropout(attention_weight_prob)\n",
    "\n",
    "        #Multiply probabilities with V to compute context\n",
    "        # result shape = (B, n_head, T, d_head)\n",
    "        context = torch.matmul(attention_weight_prob, v)\n",
    "\n",
    "        if self.log_shape:\n",
    "            print(\"weights:\", attention_weight_prob.shape, \"context:\", context.shape)\n",
    "\n",
    "        '''\n",
    "        ctx: (B, heads, T, d_head)\n",
    "        We need to combine them into:\n",
    "        (B, T, d_model) = (1, 5, 12)\n",
    "        \n",
    "        \n",
    "        1. transpose back -> (B, T, heads, d_head)\n",
    "        2. reshape to (B, T, d_model)\n",
    "        '''\n",
    "        out = context.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "\n",
    "        # linear projection\n",
    "        out = self.projection(out)\n",
    "\n",
    "        if self.log_shape:\n",
    "            print(\"Multi-Head output:\", out.shape)\n",
    "\n",
    "        return out, attention_weight_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb26dfd",
   "metadata": {},
   "source": [
    "## Second way to build the Multi Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # causal mask: lower triangular matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T, D) -> batch, token length, Dimension of one token\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        # project inputs to key, query, value\n",
    "        k = self.key(x)    # (B, T, hs)\n",
    "        q = self.query(x)  # (B, T, hs)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        attention_weight = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)   # (B, T, T)\n",
    "\n",
    "        # apply causal mask to prevent looking ahead\n",
    "        attention_weight = attention_weight.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # softmax to get attention probabilities\n",
    "        attention_weight_prob = F.softmax(attention_weight, dim=-1)   # (B, T, T)\n",
    "        attention_weight_prob = self.dropout(attention_weight_prob)\n",
    "\n",
    "        # weighted sum of values\n",
    "        v = self.value(x)                               # (B, T, hs)\n",
    "        out = attention_weight_prob @ v                  # (B, T, hs)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "#      nn.Sequential: layers are applied in order, automatically in forward.\n",
    "#      nn.ModuleList: just stores the layers, but you decide manually in forward how to use them (more flexible).\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multiple heads of self-Attention in parallel '''\n",
    "\n",
    "    def __init__(self, num_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "        self.proj = nn.Linear(num_head*head_size , n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edbd4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 13\n",
    "d_model = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tâ€¢\tToken embeddings: Each token in the vocabulary has a unique learnable vector. Shape = (vocab_size, d_model).\n",
    "\tâ€¢\tExample: \"apple\" and \"orange\" get completely different learned embeddings.\n",
    "\tâ€¢\tPositional embeddings: Each position in the sequence (0, 1, 2, â€¦, context_length-1) has a unique vector. Shape = (context_length, d_model).\n",
    "\tâ€¢\tExample: position 0 always has the same embedding vector, regardless of which token is there.\n",
    "\tâ€¢\tIt is not per-token, its per-position.\n",
    " \n",
    " only the context length positions are trained for positional embeddings, not every token individually\n",
    "'''\n",
    "\n",
    "\n",
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab: int , d_model: int, device):\n",
    "        super().__init__()\n",
    "        self.pos_embeddings = nn.Embedding(context_length,d_model, device=device)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        pos_emb = self.pos_embeddings(x)\n",
    "        return pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e26795a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = PositionalEmbeddings(vocab,d_model,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd0e77",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb723303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: ['\\n', ' ', '(', ')', ',', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'Ã­', 'à¤', 'à¤‚', 'à¤ƒ', 'à¤…', 'à¤†', 'à¤‡', 'à¤ˆ', 'à¤‰', 'à¤Š', 'à¤', 'à¤', 'à¤“', 'à¤”', 'à¤•', 'à¤–', 'à¤—', 'à¤˜', 'à¤™', 'à¤š', 'à¤›', 'à¤œ', 'à¤', 'à¤ž', 'à¤Ÿ', 'à¤ ', 'à¤¡', 'à¤¢', 'à¤£', 'à¤¤', 'à¤¥', 'à¤¦', 'à¤§', 'à¤¨', 'à¤ª', 'à¤«', 'à¤¬', 'à¤­', 'à¤®', 'à¤¯', 'à¤°', 'à¤²', 'à¤µ', 'à¤¶', 'à¤·', 'à¤¸', 'à¤¹', 'à¤¼', 'à¤½', 'à¤¾', 'à¤¿', 'à¥€', 'à¥', 'à¥‚', 'à¥ƒ', 'à¥‡', 'à¥ˆ', 'à¥‹', 'à¥Œ', 'à¥', 'à¥¤', 'à¥¥', 'à¥¦', 'â€“']\n",
      "Length of unique chars: 80\n",
      "Encoded: [49, 54, 61, 75, 45, 71]\n",
      "Decoded: à¤¨à¤®à¤¸à¥à¤¤à¥‡\n"
     ]
    }
   ],
   "source": [
    "## simple tokenizer\n",
    "\n",
    "class byteTokenizer:\n",
    "    def __init__(self, file_path: str):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            self.text = f.read()\n",
    "        \n",
    "        # Create sorted list of unique characters\n",
    "        self.chars = sorted(set(self.text))\n",
    "        print(\"Unique characters:\", self.chars)\n",
    "        print(f\"Length of unique chars: {len(self.chars)}\")\n",
    "        \n",
    "        # Mapping: char -> int and int -> char\n",
    "        self.char_to_int = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.int_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self)-> int:\n",
    "        return len(self.chars)\n",
    "        \n",
    "        \n",
    "    def encode(self, s: str) -> list[int]:\n",
    "        \"\"\"Convert string to list of integers\"\"\"\n",
    "        return [self.char_to_int[c] for c in s]\n",
    "\n",
    "    def decode(self, l: list[int]) -> str:\n",
    "        \"\"\"Convert list of integers back to string\"\"\"\n",
    "        return ''.join([self.int_to_char[i] for i in l])\n",
    "\n",
    "\n",
    "tokenizer = byteTokenizer(\"/Users/abhaykumarsingh/Desktop/Gpt/Data/tulsidas.txt\")\n",
    "encoded = tokenizer.encode(\"à¤¨à¤®à¤¸à¥à¤¤à¥‡\")  \n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab171bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Decoder_Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, n_head: int, d_model:int, dropout:float=0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0   # should be divisible (d_model % n_head = 0)\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor) -> None:\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).view(B, T, 3, self.n_head, self.d_head)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout.p if self.training else 0.0, is_causal=True)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "    \n",
    "## End Feed Forward of One Block \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, hidden_times: int = 3, dropout: float=0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_times*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_times*d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "        \n",
    "## one single tranformer block       \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dropout: float=0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.lr1Norm = nn.LayerNorm(d_model)\n",
    "        self.multiHead_attention = Decoder_Multi_Head_Attention(n_head, d_model, dropout)\n",
    "        self.lr2Norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.multiHead_attention(self.lr1Norm(x))\n",
    "        x = x + self.ffn(self.lr2Norm(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "## multi block architecture - GPT\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, context_length: int, n_block: int = 4, n_head: int = 4, d_model: int = 256, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.embedings = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.pos_embedings = nn.Embedding(context_length, d_model)\n",
    "        self.blocks = nn.Sequential(*[Block(d_model, n_head, dropout) for _ in range(n_block)])\n",
    "        self.out_head = nn.Linear(d_model, vocab_size)\n",
    "        self.lrNorm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    # initialising the weights in normal distribution near zero - this results better than random initialising \n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std= 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    \n",
    "    \n",
    "        \n",
    "    def forward(self, idx: torch.Tensor, targets: Optional[torch.Tensor]=None):\n",
    "        B, T = idx.shape\n",
    "        if T > self.context_length:\n",
    "            idx = idx[:, -self.context_length:]\n",
    "            T = idx.size(1)\n",
    "        \n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.embedings(idx) + self.pos_embedings(pos)\n",
    "        x = self.blocks(x)\n",
    "        x = self.lrNorm(x)\n",
    "        logits = self.out_head(x)\n",
    "        loss = None\n",
    "        if targets is not None :\n",
    "            loss = F.cross_entropy(logits.flatten(0, 1), targets.flatten())\n",
    "            \n",
    "        return logits, loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
